{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDiscriminative connection identification\n========================================\n**What you'll learn**: Build a model\naround the weight of a linear classifier\nto identify the discriminant connections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Author**: `Dhaif BEKHA <dhaif@dhaifbekha.com>`_\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve the example dataset\n----------------------------\n\nIn this example, we will work directly on a pre-computed dictionary,\nthat contain two set of connectivity matrices, from two different groups.\nThe first group, called *controls* is a set of connectivity matrices from healthy\nseven years old children, and the second group called *patients*, is a set of\nconnectivity matrices from seven years old children who have suffered a stroke.\nYou can download the dictionary use in this example\n`here <https://www.dropbox.com/s/kwdrx4liauo10kr/raw_subjects_connectivity_matrices.pkl?dl=1>`_.\nFinally, we will plot some results on a glass brain, and we will need\nthe nodes coordinates of the atlas regions in which the signals were extracted.\nYou can download the `atlas <https://www.dropbox.com/s/wwmg0a4g3cjnfvv/atlas.nii?dl=1>`_,\nand the corresponding `labels <https://www.dropbox.com/s/3wuzwn14l7nksvy/atlas_labels.csv?dl=1>`_.\nAs usual, we will suppose that all needed files are in your **home directory**.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modules import\n--------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport os\nfrom conpagnon.utils.folders_and_files_management import load_object\nfrom conpagnon.machine_learning.features_indentification import discriminative_brain_connection_identification, \\\n    find_top_features\nimport numpy as np\nfrom conpagnon.data_handling import atlas\nfrom conpagnon.utils.array_operation import array_rebuilder\nfrom nilearn.connectome import sym_matrix_to_vec\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nimport pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load data\n---------\n\nWe load the **subjects connectivity matrices** dictionary,\nand the **atlas** that you previously downloaded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fetch the path of the home directory\nhome_directory = str(Path.home())\n# Filename of the atlas file.\natlas_file_name = 'atlas.nii'\n# Full path to atlas labels file\natlas_label_file = os.path.join(home_directory, 'atlas_labels.csv')\n# Set the colors of the twelves network in the atlas\ncolors = ['navy', 'sienna', 'orange', 'orchid', 'indianred', 'olive',\n          'goldenrod', 'turquoise', 'darkslategray', 'limegreen', 'black',\n          'lightpink']\n# Number of regions in each of the network\nnetworks = [2, 10, 2, 6, 10, 2, 8, 6, 8, 8, 6, 4]\n# We can call fetch_atlas to retrieve useful information about the atlas\natlas_nodes, labels_regions, labels_colors, n_nodes = atlas.fetch_atlas(\n    atlas_folder=home_directory,\n    atlas_name=atlas_file_name,\n    network_regions_number=networks,\n    colors_labels=colors,\n    labels=atlas_label_file,\n    normalize_colors=True)\n\n# We now fetch the subjects connectivity dictionary\n\n# Load the dictionary containing the connectivity matrices\nsubjects_connectivity_matrices = load_object(\n    full_path_to_object=os.path.join(home_directory, 'raw_subjects_connectivity_matrices.pkl'))\n# Groups names in the dictionary\ngroups = list(subjects_connectivity_matrices.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understand and analyze the results of a classifier\n--------------------------------------------------\n\nIn the basic section, we saw an example of **two groups classification**\nusing a classifier, a **S**upport **V**ector **M**achine (SVM) with a\nlinear kernel. Please, do not hesitate to go back to the two groups\nclassification tutorial if needed. Briefly, with a SVM we perform\na classification task between patients and controls, with a\nStratified and Shuffle cross-validation scheme. This task\nis performed for the correlation, partial correlation and\ntangent space connectivity matrices. We compute and store\nthe accuracy for each of those connectivity metric.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Labels vectors: 0 for the first class, 1 for the second. Those\n# 1, and 0 are the label for each subjects.\nclass_labels = np.hstack((np.zeros(len(subjects_connectivity_matrices[groups[0]].keys())),\n                          np.ones(len(subjects_connectivity_matrices[groups[1]].keys()))))\n\n# total number of subject\nn_subjects = len(class_labels)\n\n# Stratified Shuffle and Split cross validation:\n# we initialize the cross validation object\n# and set the split to 10000.\ncv = StratifiedShuffleSplit(n_splits=10000,\n                            random_state=0)\n\n# Instance initialization of SVM classifier with a linear kernel\nsvc = LinearSVC()\n\n# Compare the classification accuracy across multiple metric\nmetrics = ['tangent', 'correlation', 'partial correlation']\nmean_scores = []\n\n# To decrease the computation time you\n# can distribute the computation on\n# multiple core, here, 4.\nn_jobs = 4\n\n# Final mean accuracy scores will be stored in a dictionary\nmean_score_dict = {}\nfor metric in metrics:\n    # We take the lower triangle of each matrices, and vectorize it to\n    # produce a classical machine learning data array of shape (n_subjects, n_features)\n    features = sym_matrix_to_vec(np.array([subjects_connectivity_matrices[group][subject][metric]\n                                           for group in groups\n                                           for subject in subjects_connectivity_matrices[group].keys()],\n                                          ), discard_diagonal=True)\n    print('Evaluate classification performance on {} with '\n          '{} observations and {} features...'.format(metric, n_subjects, features.shape[1]))\n    # We call cross_val_score, a convenient scikit-learn\n    # wrapper that will perform the classification task\n    # with the desired cross-validation scheme.\n    cv_scores = cross_val_score(estimator=svc, X=features,\n                                y=class_labels, cv=cv,\n                                scoring='accuracy', n_jobs=n_jobs)\n    # We compute the mean cross-validation score\n    # over all the splits\n    mean_scores.append(cv_scores.mean())\n    # We store the mean accuracy score\n    # in a dictionary.\n    mean_score_dict[metric] = cv_scores.mean()\n    print('Done for {}'.format(metric))\n\n# Lets print out the accuracy for each metric\nfor metric in metrics:\n    print('{} accuracy: {} %'.format(metric, mean_score_dict[metric] * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We input a set of ``2556`` features, i.e, the connectivity coefficients,\nto the classifier. In the training phase, the classifier give a\n**weight** to each input features. It's very common that\nthe features have not the same importance in the decision\nwhether for a given subject it's a patient or a control\nsubject. If we simplify, some feature are \"not so important\"\nin the final decision, and they have a \"small weight\" compared\nto some other feature that are more important in the final\ndecision, with higher weight. The idea of the algorithm\nthat we detailed below is explore the classifier weight\nand build a statistic for those weight to find the\nmore important one.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. important::\n  We will use the **tangent space** metric\n  only for the rest of this tutorial. The tangent\n  space metric is better suited for classification\n  task, and in general, to pinpoint individual\n  difference in functional connectivity. We detailed\n  why in another tutorial of the advanced examples.\n  Do not hesitate to dive deeper in the subject if\n  you need to.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discriminative connection identification: the algorithm\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe presented algorithm was created by Bertrand Ng & al,\nand we encourage you to read the full paper with a deeper\ndive in the theory\n`here <https://www.dropbox.com/s/9z3gg8uqnmwvcl6/Transport%20on%20Riemannian%20Manifold.pdf?dl=1>`_.\nLet's consider we have ${ d\\Sigma^{1} ... d\\Sigma^{S} }$, the tangent matrices\nof the whole cohort (patients and controls), with the corresponding labels vector\n$[0, 0, ..., 1, 1 ,1]$, coding $0$ for a control subject, and $1$\nfor a patient. We first compute the classification for this set of tangent matrices\nand labels, which results in a vector of weights,  $\\omega$. We then,\n**randomly permute**  $N$ times the labels vector. Then, for each\npermutation, we perform the classifier learning on a bootstrapped sample\nof size  $B$. So, for each permutation, for each bootstrapped sample\nwe compute the classifier weights  $\\omega_{n, b}$ with\n$b \\in {1, ... B}$ and  $n \\in {1, ... N}$. For each\npermutation we ended up with a matrix weight of size  $(B, n_{features})$.\nWe then compute the **normalized weight over the bootstrapped sample**:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\\Omega_{n} = \\frac{1}{B} \\frac{\\sum_{b=1}^{B}\\omega_{n, b}}{std(\\omega_{n, b})}\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then, store the **maximum** normalized weight for the permutation $n$.\nThis is the **null distribution of maximum normalized weight mean**.\nAfter the  $N$ permutation we compute\n$\\Omega_{0} = \\frac{1}{B} \\frac{\\sum_{b=1}^{B}\\omega_{b}}{std(\\omega_{b})}$\nwhich is simply the normalized weight without permutations. Giving the We finally declare a weight\nof $\\Omega_{0}$ significant if they are greater than the 99 th percentile of the\nnull distribution, corresponding to a p-value threshold of 0.01. We use the same\nclassifier as the classification task above, that is, a SVM with a linear kernel.\nWe keep the regularization term **C** to the default value one.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>For the **negative weights**, we store the **minimum**\n  of the normalized weight mean instead of the maximum.\n  In the end, you will end up with a set of significant\n  negative weight, and a set of positive one. This\n  way of assessing significance is often called\n  maxT (or minT)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We build a function that conveniently wrap all the\ndetailed step above,\n:py:func:`conpagnon.machine_learning.features_indentification.discriminative_brain_connection_identification`.\nThis function parallelize the computation of the bootstrapped weights. In this function, you can also\nchoose a parametric correction like FDR for example. I encourage you to look up to the docstring before\nusing it. We write all the useful results in a report, including the top weight labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Stacked the 2D array of connectivity matrices for each subjects\nclass_names = groups\nmetric = 'tangent'\n\n# Vectorize the connectivity matrices\nvectorized_connectivity_matrices = sym_matrix_to_vec(\n    np.array([subjects_connectivity_matrices[class_name][s][metric] for class_name\n              in class_names for s in subjects_connectivity_matrices[class_name].keys()]),\n    discard_diagonal=True)\n\n# Stacked connectivity matrices\nstacked_connectivity_matrices = np.array([subjects_connectivity_matrices[class_name][s][metric]\n                                          for class_name in class_names\n                                          for s in subjects_connectivity_matrices[class_name].keys()])\n\n# Compute mean connectivity matrices for each class\n# for plotting purpose only\nfirst_class_mean_matrix = np.array([subjects_connectivity_matrices[class_names[0]][s][metric] for s in\n                                    subjects_connectivity_matrices[class_names[0]].keys()]).mean(axis=0)\nsecond_class_mean_matrix = np.array([subjects_connectivity_matrices[class_names[1]][s][metric] for s in\n                                     subjects_connectivity_matrices[class_names[1]].keys()]).mean(axis=0)\n\n# Directory where you wan to save the report\nsave_directory = home_directory\n\n# Labels vectors\nclass_labels = np.hstack((1*np.ones(len(subjects_connectivity_matrices[class_names[0]].keys())),\n                          -1*np.ones(len(subjects_connectivity_matrices[class_names[1]].keys()))))\n\nclassifier_weights, weight_null_distribution, p_values_corrected = \\\n    discriminative_brain_connection_identification(\n        vectorized_connectivity_matrices=vectorized_connectivity_matrices,\n        class_labels=class_labels,\n        class_names=class_names,\n        save_directory=save_directory,\n        n_permutations=100,\n        bootstrap_number=200,\n        features_labels=labels_regions,\n        features_colors=labels_colors,\n        n_nodes=n_nodes,\n        atlas_nodes=atlas_nodes,\n        first_class_mean_matrix=first_class_mean_matrix,\n        second_class_mean_matrix=second_class_mean_matrix,\n        n_cpus_bootstrap=8,\n        top_features_number=10,\n        write_report=True,\n        correction='fdr_bh',\n        C=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. danger::\n  For the sake of computation time, we\n  choose to compute 500 permutation\n  on bootstrapped sample of size 200.\n  We recommend that you runs at least\n  10,000 permutation on 500 bootstrapped\n  sample size. We also choose a less conservative\n  correction than the original algorithm, a FDR\n  correction dealing with the multiple comparison.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will not recreate exactly all the plot and information\nyou'll find in the final report, but in the section\nbelow we will first plot the histogram of the top-weight\nfor both positive and negative weights. The full report\nis stored in your home directory, and contains all the\nfigure, along with a text file with all the analytics\nyou'll need.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find top features\n# Rebuild a symmetric array from normalized mean weight vector\nnormalized_mean_weight_array = array_rebuilder(classifier_weights,\n                                               'numeric', diagonal=np.zeros(n_nodes))\nnormalized_mean_weight_array_top_features, top_weights, top_coefficients_indices, top_weight_labels = \\\n    find_top_features(normalized_mean_weight_array=normalized_mean_weight_array,\n                      labels_regions=labels_regions,\n                      top_features_number=19)\n\n# Convert the array of labels regions into\n# a list of string for plotting purpose.\nlabels_str = []\nfor i in range(top_weight_labels.shape[0]):\n    labels_str.append(str(top_weight_labels[i]))\n\n# let's plot the top negative and positive weight\n# on a bar plot\nweight_colors = ['blue' if weight < 0 else 'red' for weight in top_weights]\nplt.bar(np.arange(len(top_weights)), list(top_weights),\n        color=weight_colors,\n        edgecolor='black',\n        alpha=0.5)\nplt.xticks(np.arange(0, len(top_weights)), labels_str,\n           rotation=60,\n           ha='right')\nfor label in range(len(plt.gca().get_xticklabels())):\n    plt.gca().get_xticklabels()[label].set_color(weight_colors[label])\nplt.xlabel('Features names')\nplt.ylabel('Features weights')\nplt.title('Top {} features ranking of normalized mean weight'.format(20))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n  The barplot above give you\n  a sense of what will be the\n  import feature, i.e, **pairs\n  of regions**, that will have\n  a huge impact on the classification\n  decision. For example, it seems that\n  the connectivity between the left and\n  right supra-marginal gyrus and also\n  for the intra-parietal sulcus plays\n  an important role in the classification\n  decision.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the text file generated\nby the analysis. The text file\nshould in your home directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the text file:\ntext_filename = 'features_identification_patients_controls_0.05_fdr_bh.txt'\ndiscriminative_connection_identification_report = pd.read_table(os.path.join(home_directory, text_filename))\nprint(discriminative_connection_identification_report.to_markdown())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the text report, you find the full list of the\nregions that are the most important in the\ndecision, that is, the regions which the\np-value associated to the attached weight\nthat are significant. In the **pdf report**\nyou will find a glass brain plot with\nthe significant positive and negative\nregions.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}